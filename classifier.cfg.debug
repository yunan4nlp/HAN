[Data]
pretrained_embeddings_file = emb/emb.sample
data_dir = legal_data
train_file = %(data_dir)s/sample_combine_raw_norm
dev_file = %(data_dir)s/sample_combine_raw_norm
test_file = %(data_dir)s/sample_combine_raw_norm
min_occur_count = 0

[Save]
save_dir = new-classifer-model
config_file = %(save_dir)s/config.cfg
save_encoder_path = %(save_dir)s/encoder
save_decoder_path = %(save_dir)s/decoder
save_vocab_path = %(save_dir)s/vocab
load_dir = new-classifer-model
load_encoder_path = %(load_dir)s/encoder
load_decoder_path = %(load_dir)s/decoder
load_vocab_path = %(load_dir)s/vocab

[Network]
# LSTMEncoder HANEncoder CHANEncoder
encoder = CHANEncoder
lstm_layers = 1
role_dims = 100
word_dims = 100
tag_dims = 100
dropout_emb = 0.33
lstm_hiddens = 100
dropout_lstm_input = 0.33
dropout_lstm_hidden = 0.33
mlp_arc_size = 500
mlp_rel_size = 100
dropout_mlp = 0.33

[Optimizer]
learning_rate = 2e-3
decay = .75
decay_steps = 5000
beta_1 = .9
beta_2 = .9
epsilon = 1e-12
clip = 5.0

[Run]
max_turn_length = 140
max_sent_length = 55
num_buckets_train = 40
num_buckets_valid = 10
num_buckets_test = 10
train_iters = 50000
train_batch_size = 100
test_batch_size = 50
validate_every = 200
save_after = 1
update_every = 4

